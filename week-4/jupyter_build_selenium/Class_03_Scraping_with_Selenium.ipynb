{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Data Scraping\n",
    "\n",
    "[Spring 2023 ITSS Mini-Course](https://www.colorado.edu/cartss/programs/interdisciplinary-training-social-sciences-itss/mini-course-web-data-scraping) — ARSC 5040  \n",
    "[Brian C. Keegan, Ph.D.](http://brianckeegan.com/)  \n",
    "[Assistant Professor, Department of Information Science](https://www.colorado.edu/cmci/people/information-science/brian-c-keegan)  \n",
    "University of Colorado Boulder  \n",
    "\n",
    "Copyright and distributed under an [MIT License](https://opensource.org/licenses/MIT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets us talk to servers on the web\n",
    "import requests\n",
    "\n",
    "# Parsing HTML magic\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# For data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Will be helful for converting between timestamps\n",
    "from datetime import datetime\n",
    "\n",
    "# We want to sleep from time-to-time to avoid overwhelming another server\n",
    "import time\n",
    "\n",
    "# We'll need to parse some strings, so we'll write some regular expressions\n",
    "import re\n",
    "\n",
    "from urllib.parse import quote, unquote\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "driver = webdriver.Remote(\n",
    "    command_executor='http://172.18.0.4:5555/wd/hub',\n",
    "    options=webdriver.ChromeOptions()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google\n"
     ]
    }
   ],
   "source": [
    "driver.get('https://www.google.com')\n",
    "print(driver.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block of code below will only work once you've installed Selenium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "WebDriverException",
     "evalue": "Message: GET /session/d9f11f0f60c7b26987f7b054c1f27284/source\nBuild info: version: '3.14.0', revision: 'aacccce0', time: '2018-08-02T20:13:22.693Z'\nSystem info: host: '8d0d73b8c984', ip: '172.18.0.4', os.name: 'Linux', os.arch: 'amd64', os.version: '6.10.14-linuxkit', java.version: '1.8.0_181'\nDriver info: driver.version: unknown\nStacktrace:\n    at org.openqa.selenium.remote.http.AbstractHttpCommandCodec.decode (AbstractHttpCommandCodec.java:261)\n    at org.openqa.selenium.remote.http.AbstractHttpCommandCodec.decode (AbstractHttpCommandCodec.java:117)\n    at org.openqa.selenium.remote.server.ProtocolConverter.handle (ProtocolConverter.java:74)\n    at org.openqa.selenium.remote.server.RemoteSession.execute (RemoteSession.java:127)\n    at org.openqa.selenium.remote.server.WebDriverServlet.lambda$handle$3 (WebDriverServlet.java:250)\n    at java.util.concurrent.Executors$RunnableAdapter.call (Executors.java:511)\n    at java.util.concurrent.FutureTask.run (FutureTask.java:266)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run (Thread.java:748)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mWebDriverException\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdriver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpage_source\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py:570\u001b[39m, in \u001b[36mWebDriver.page_source\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    563\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpage_source\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    564\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Gets the source of the current page.\u001b[39;00m\n\u001b[32m    565\u001b[39m \n\u001b[32m    566\u001b[39m \u001b[33;03m    Example:\u001b[39;00m\n\u001b[32m    567\u001b[39m \u001b[33;03m    --------\u001b[39;00m\n\u001b[32m    568\u001b[39m \u001b[33;03m    >>> print(driver.page_source)\u001b[39;00m\n\u001b[32m    569\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m570\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGET_PAGE_SOURCE\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py:429\u001b[39m, in \u001b[36mWebDriver.execute\u001b[39m\u001b[34m(self, driver_command, params)\u001b[39m\n\u001b[32m    427\u001b[39m response = \u001b[38;5;28mself\u001b[39m.command_executor.execute(driver_command, params)\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merror_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    430\u001b[39m     response[\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m._unwrap_value(response.get(\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    431\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/selenium/webdriver/remote/errorhandler.py:232\u001b[39m, in \u001b[36mErrorHandler.check_response\u001b[39m\u001b[34m(self, response)\u001b[39m\n\u001b[32m    230\u001b[39m         alert_text = value[\u001b[33m\"\u001b[39m\u001b[33malert\u001b[39m\u001b[33m\"\u001b[39m].get(\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    231\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[31mWebDriverException\u001b[39m: Message: GET /session/d9f11f0f60c7b26987f7b054c1f27284/source\nBuild info: version: '3.14.0', revision: 'aacccce0', time: '2018-08-02T20:13:22.693Z'\nSystem info: host: '8d0d73b8c984', ip: '172.18.0.4', os.name: 'Linux', os.arch: 'amd64', os.version: '6.10.14-linuxkit', java.version: '1.8.0_181'\nDriver info: driver.version: unknown\nStacktrace:\n    at org.openqa.selenium.remote.http.AbstractHttpCommandCodec.decode (AbstractHttpCommandCodec.java:261)\n    at org.openqa.selenium.remote.http.AbstractHttpCommandCodec.decode (AbstractHttpCommandCodec.java:117)\n    at org.openqa.selenium.remote.server.ProtocolConverter.handle (ProtocolConverter.java:74)\n    at org.openqa.selenium.remote.server.RemoteSession.execute (RemoteSession.java:127)\n    at org.openqa.selenium.remote.server.WebDriverServlet.lambda$handle$3 (WebDriverServlet.java:250)\n    at java.util.concurrent.Executors$RunnableAdapter.call (Executors.java:511)\n    at java.util.concurrent.FutureTask.run (FutureTask.java:266)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run (Thread.java:748)"
     ]
    }
   ],
   "source": [
    "print(driver.page_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Selenium\n",
    "\n",
    "This is a non-trivial process: you will need to (1) install the Python bindings for Selenium, (2) download a web driver to interface with a web browser, and (3) configure Selenium to recognize your web driver. Follow the installation instructions in the documentation [here](https://selenium-python.readthedocs.io/installation.html) (you won't need the Selenium server).\n",
    "\n",
    "1. Install the Python bindings for Selenium. Go to your Anaconda terminal window, type in this command, and agree to whatever the package manager wants to install or update.\n",
    "\n",
    "`conda install selenium`\n",
    "\n",
    "2. Download the driver(s) for the web browser you want to use from the [links on the Selenium documentation](https://selenium-python.readthedocs.io/installation.html). If you use a Chrome browser, download the Chrome driver. Note that the Safari driver will not work on PCs and the Edge driver will not work on Macs. \n",
    "\n",
    "3. You will need to unzip the file and move the executable to the same directory where you are running this notebook. Make a note of the path to this directory.\n",
    "\n",
    "### Using Selenium to control a web browser\n",
    "The `driver` object we create is a connection from this Python environment out to the browser window.\n",
    "\n",
    "If you're on a Mac, the latest versions of OS X *really* do not like letting you run applications you've just downloaded. You'll need to dive into your system settings to fix it: https://support.apple.com/en-us/HT202491"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This single line of code will open a new browser window and will request the \"xkcd\" homepage.\n",
    "\n",
    "Your computer's security protocols may vigorously protest because you are launching a program that is controlled by another process/program. You will need to dismiss these warnings in order to proceed. Whether and how to do that will vary considerably across PCs and Macs, the kinds of permissions your account has on this operating system, and other security measures employed by your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<selenium.webdriver.remote.webdriver.WebDriver (session=\"d9f11f0f60c7b26987f7b054c1f27284\")>\n"
     ]
    }
   ],
   "source": [
    "driver.get('https://xkcd.com')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can use Python to control a web browser, there are a host of powerful functions you can use to simulate keystrokes, locate elements on a page, manage waits, *etc*.: https://selenium-python.readthedocs.io/\n",
    "\n",
    "In Classes 01 and 02, we used `BeautifulSoup` to turn HTML and XML into a data structure that we could search and access using Python-like syntax. With Selenium we use a standard called \"XPath\" to navigate through an HTML document: [this is the official tutorial](https://www.w3schools.com/xml/xpath_syntax.asp) for working with XPath. The syntax is different, but the intuition is similar: we can find a parent node by its attribute (class, id, *etc*.) and then navigate down the tree to its children.\n",
    "\n",
    "The XPath below has the following elements in sequence\n",
    "* `//` — Select all nodes that match the selection\n",
    "* `[@id=\"middleContainer\"]` — find the element that has a \"middleContainer\" id.\n",
    "* `/ul[2]` — select the second `<ul>` element underneath the `<div id=\"middleContainer\">`\n",
    "* `/li[3]` — select the third `<li>` element \n",
    "* `/a` — select the a element\n",
    "\n",
    "The combined XPath string `//*[@id=\"middleContainer\"]/ul[1]/li[3]/a` is like a \"file directory\" that (hopefully!) points to the hyperlink button that takes us to a random xkcd comic. With the directions to this button, we can have the web browser \"click\" the \"Random\" button beneath the comic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"d9f11f0f60c7b26987f7b054c1f27284\", element=\"0.8119973885642442-1\")>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's find the 'random' buttom\n",
    "element = driver.find_element(By.XPATH,'//*[@id=\"middleContainer\"]/ul[2]/li[3]/a')\n",
    "\n",
    "# Once we've found it, now click it\n",
    "element.click()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the attributes of different parts of the web page. xkcd is famous for its \"hidden messages\" inside the image alt-text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'They had BETTER make this a sample return mission.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alttext_element = driver.find_element(By.XPATH,'//*[@id=\"comic\"]/img')\n",
    "alttext_element.get_attribute(\"title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could write a simple loop to click on the random button five times and print the alt-text from each of those pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " This projection distorts both area and direction, but preserves Melbourne.\n",
      "\n",
      " Kind of rude of them to simultaneously issue an EVACUATION - IMMEDIATE alert, a SHELTER IN PLACE alert, and a 911 TELEPHONE OUTAGE alert.\n",
      "\n",
      " 1. Nf3 ... ↘↘↘  2. Nc3 ... ↘↘↘  0-1\n",
      "\n",
      " The place I'd least like to live is the farm in the background of those diagrams showing how tornadoes form.\n",
      "\n",
      " Our investigation into whining-based remedies became the first study to be halted by the IRB on the grounds that the treatment group was 'too annoying.'\n"
     ]
    }
   ],
   "source": [
    "for c in range(5):\n",
    "    time.sleep(2)\n",
    "    random_element = driver.find_element(By.XPATH,'//*[@id=\"middleContainer\"]/ul[2]/li[3]/a')\n",
    "    random_element.click()\n",
    "    \n",
    "    alttext_element = driver.find_element(By.XPATH,'//*[@id=\"comic\"]/img')\n",
    "    print('\\n',alttext_element.get_attribute(\"title\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you're done playing with your programmable web browser, make sure to close it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that with the connection to the web browser closed, any of the functions like `find_element_by_xpath`, `click()`, *etc*. will not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "Start your driver again and get the xkcd homepage.\n",
    "\n",
    "1. Change the XPath to click on the \"Prev\" button above the comic.\n",
    "2. Change the XPath to search for the \"comicNav\" class instead of the \"middleContainer\" id.\n",
    "3. Change the XPath to click on the \"About\" button in the upper-left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warning!\n",
    "\n",
    "The code we will write and execute below will violate the Terms of Service for [Twitter](https://twitter.com/en/tos) (\"You may not...  access or search or attempt to access or search the Services by any means (automated or otherwise) other than through our currently available, published interfaces that are provided by Twitter\") and [YouTube](https://www.youtube.com/static?template=terms) (\"you are not allowed to... access the Service using any automated means (such as robots, botnets or scrapers)...\") for retrieving information from the platform. In effect, we will transmit code in excess of our authorized access and potentially cause damage, in order to obtain information from a protected computer. \n",
    "\n",
    "We will do this in order to obtain public statements made by goverment officials acting in their official capacity because this data is otherwise unavailable for retrieval from YouTube. There is an interesting body of emerging legal precedent treating elected officials' use of Twitter as a public forum: [*Knight First Amendment Institute v. Trump*](https://en.wikipedia.org/wiki/Knight_First_Amendment_Institute_v._Trump) established that [the President may not block other Twitter users](https://www.courtlistener.com/docket/6087955/72/knight-first-amendment-institute-at-columbia-university-v-trump/):\n",
    "\n",
    "> * \"We hold that portions of the @realDonaldTrump account -- the “interactive space” where Twitter users may directly engage with the content of the President’s tweets -- are properly analyzed under the “public forum” doctrines set forth by the Supreme Court, that such space is a designated public forum...\"\n",
    "> * \"we nonetheless conclude that the extent to which the President and Scavino can, and do, exercise control over aspects of the @realDonaldTrump account are sufficient to establish the government-control element as to the content of the tweets sent by the @realDonaldTrump account, the timeline compiling those tweets, and the interactive space associated with each of those tweets.\"\n",
    "> * \"Because a Twitter user lacks control over the comment thread beyond the control exercised over first-order replies through blocking, the comment threads -- as distinguished from the content of tweets sent by @realDonaldTrump, the @realDonaldTrump timeline, and the interactive space associated with each tweet -- do not meet the threshold criterion for being a forum.\"\n",
    "> * \"the account’s timeline, which “displays all tweets generated by the [account]”... all of which is government speech.\"\n",
    "\n",
    "On this basis, I believe the White House's videos posted to Twitter or YouTube are government speech and our automated retrieval of this content and associated meta-data in violation of YouTube's Terms of Serice is justifiable for understanding this speech as a public forum.\n",
    "\n",
    "I would advise you against using these tools and approaches without a similarly clear public interest rationale and jurisprudence linking behavior to public forum doctrines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Screen-scraping Twitter with Selenium\n",
    "\n",
    "I am adapting a [tutorial by Shawn Wang](https://dev.to/swyx/scraping-my-twitter-social-graph-with-python-and-selenium--hn8) on scraping a Twitter graph with Python and Selenium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the Chrome driver for my PC -- yours is likely very different\n",
    "# driver = selenium.webdriver.Chrome(executable_path='E:/Dropbox/Courses/2019 Spring - ITSS Web Data Scraping/chromedriver.exe')\n",
    "\n",
    "# Path to the Chrome driver for my Mac -- yours is likely very different\n",
    "driver = selenium.webdriver.Firefox(executable_path=mac_path)\n",
    "\n",
    "driver.get('https://www.twitter.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually log in to your Twitter account through the driver page.\n",
    "\n",
    "Then go to the \"followings\" (or followees, also called \"friends\" in the Twitter API) of an account. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://twitter.com/JoeBiden/following')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the time of this Notebook's writing, the \"JoeBiden\" account followed 47 other accounts. Depending on the resolution of your display, size of the window, *etc*. there may only be 10–20 accounts visible. We can scroll to see the rest of these accounts programatically.\n",
    "\n",
    "Run this cell a few times to keep scrolling to the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass the HTML of the web page in the browser back to Python and turn it into soup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = driver.page_source.encode('utf-8')\n",
    "\n",
    "soup = BeautifulSoup(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, since I last ran this class in 2019 Twitter changed how they design and populate their website. If we inspect the elements for the following, we can see how they obfuscate the elements to make them hard to scrape. \n",
    "\n",
    "*Back in the day*, they had nice div tags with \"data-item-type\"s called \"user\". No longer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.body.find_all('div', attrs={'data-item-type':'user'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Screen-scraping YouTube with Selenium\n",
    "\n",
    "Let's get data from YouTube instead, which appears to be better-behaved from a web scraper's perspective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.youtube.com/c/whitehouse/videos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YouTube like Twitter also loads additional videos on scroll, so let's re-use the code above to scroll until we can't to load as many videos as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/51345544\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    time.sleep(.1)\n",
    "    html.send_keys(Keys.END)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've loaded as many videos as the YouTube interface allows by scrolling, pull the contents of the web page. We can revert back to our strategies from Week 2 to identify, navigate, and pull out the relevant fields we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = driver.page_source.encode('utf-8')\n",
    "\n",
    "soup = BeautifulSoup(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inspecting the source, the video cells appear to live within elements called `<ytd-grid-video-renderer>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_divs = soup.find_all('ytd-grid-video-renderer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_divs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further drill-down and inspection of this element from the browser reveals that some promising data lives within an element defined as `<a id=\"video-title>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_divs[-1].find_all('a',{'id':'video-title'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the `aria-label` string there's the title, account, a relative date and a detailed number of views. Within the `href` tag is the video ID. These all seem like promising bits of data to try to grab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_divs[-1].find_all('a',{'id':'video-title'})[0]['aria-label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use regular expressions to try to match the numeric fields like the number of views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_s = video_divs[-1].find_all('a',{'id':'video-title'})[0]['aria-label']\n",
    "re.findall(r'([\\d,]+) views',_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract other relevant fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The video link\n",
    "video_divs[-1].find_all('a',{'id':'video-title'})[0]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The video title\n",
    "video_divs[-1].find_all('a',{'id':'video-title'})[0]['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's also some helpful data about the length of the video hiding in a `<ytd-thumbnail-overlay-time-status-renderer>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_divs[-1].find_all('ytd-thumbnail-overlay-time-status-renderer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pull out that video length element with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_divs[-1].find_all('ytd-thumbnail-overlay-time-status-renderer')[0].text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's also labels about whether the videos are closed captioned within a tag called `<span class=\"style-scope ytd-badge-supported-renderer\">`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_divs[-1].find_all('span',{'class':'style-scope ytd-badge-supported-renderer'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first video (at the bottom) has a CC tag, the second one does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_divs[-2].find_all('span',{'class':'style-scope ytd-badge-supported-renderer'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put these pieces together into a loop that grabs all the data from this list of videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_l = []\n",
    "\n",
    "for d in video_divs:\n",
    "    # Get the number of views\n",
    "    _s = d.find_all('a',{'id':'video-title'})[0]['aria-label']\n",
    "    _views = re.findall(r'([\\d,]+) views',_s)[0]\n",
    "    \n",
    "    # Get the link\n",
    "    _link = d.find_all('a',{'id':'video-title'})[0]['href']\n",
    "    \n",
    "    # Get the title\n",
    "    _title = d.find_all('a',{'id':'video-title'})[0]['title']\n",
    "    \n",
    "    # Get the length\n",
    "    _length = d.find_all('ytd-thumbnail-overlay-time-status-renderer')[0].text.strip()\n",
    "    \n",
    "    # Get the captioning\n",
    "    if len(d.find_all('span',{'class':'style-scope ytd-badge-supported-renderer'})) > 1:\n",
    "        _cc = True\n",
    "    else:\n",
    "        _cc = False\n",
    "        \n",
    "    # Package it all up into a dictionary\n",
    "    _d = {'Views':_views,\n",
    "          'Link':_link,\n",
    "          'Title':_title,\n",
    "          'Length':_length,\n",
    "          'Captioned':_cc\n",
    "         }\n",
    "    \n",
    "    # Add our dictionary to the container\n",
    "    videos_l.append(_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the `videos_l` container into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(videos_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your priorities, you could stop here and save this to a CSV since there is already rich data. \n",
    "\n",
    "Some limitations to think of include:\n",
    "* How would YouTube serve up an account with hundreds or thousands of videos? Is there a limit to the videos you can get from scrolling?\n",
    "* The \"Views\" columns are stored as strings not as numeric values: you'll want to convert them somehow. Those commas could also complicate things when storing as a *comma separated* file, so you'll want to strip them out too somehow. Fixing both of these are related.\n",
    "* We don't have any of the valuable data about the actual date the video was posted, the number of up/down votes, or even the transcript from the captioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving data from each video's page\n",
    "\n",
    "There's valuable data on each video's page about the specific date, the up and down votes, and even the transcript of the video that we can also retrieve.\n",
    "\n",
    "Let's start with the inauguration video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.youtube.com/watch?v=q5iCPKDp4V4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the raw markdown and soup-ify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "WebDriverException",
     "evalue": "Message: GET /session/d9f11f0f60c7b26987f7b054c1f27284/source\nBuild info: version: '3.14.0', revision: 'aacccce0', time: '2018-08-02T20:13:22.693Z'\nSystem info: host: '8d0d73b8c984', ip: '172.18.0.4', os.name: 'Linux', os.arch: 'amd64', os.version: '6.10.14-linuxkit', java.version: '1.8.0_181'\nDriver info: driver.version: unknown\nStacktrace:\n    at org.openqa.selenium.remote.http.AbstractHttpCommandCodec.decode (AbstractHttpCommandCodec.java:261)\n    at org.openqa.selenium.remote.http.AbstractHttpCommandCodec.decode (AbstractHttpCommandCodec.java:117)\n    at org.openqa.selenium.remote.server.ProtocolConverter.handle (ProtocolConverter.java:74)\n    at org.openqa.selenium.remote.server.RemoteSession.execute (RemoteSession.java:127)\n    at org.openqa.selenium.remote.server.WebDriverServlet.lambda$handle$3 (WebDriverServlet.java:250)\n    at java.util.concurrent.Executors$RunnableAdapter.call (Executors.java:511)\n    at java.util.concurrent.FutureTask.run (FutureTask.java:266)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run (Thread.java:748)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mWebDriverException\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m driver.get(\u001b[33m\"\u001b[39m\u001b[33mhttps://www.geeksforgeeks.org/\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mdriver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpage_source\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py:570\u001b[39m, in \u001b[36mWebDriver.page_source\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    563\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpage_source\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    564\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Gets the source of the current page.\u001b[39;00m\n\u001b[32m    565\u001b[39m \n\u001b[32m    566\u001b[39m \u001b[33;03m    Example:\u001b[39;00m\n\u001b[32m    567\u001b[39m \u001b[33;03m    --------\u001b[39;00m\n\u001b[32m    568\u001b[39m \u001b[33;03m    >>> print(driver.page_source)\u001b[39;00m\n\u001b[32m    569\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m570\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGET_PAGE_SOURCE\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py:429\u001b[39m, in \u001b[36mWebDriver.execute\u001b[39m\u001b[34m(self, driver_command, params)\u001b[39m\n\u001b[32m    427\u001b[39m response = \u001b[38;5;28mself\u001b[39m.command_executor.execute(driver_command, params)\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merror_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    430\u001b[39m     response[\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m._unwrap_value(response.get(\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    431\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/selenium/webdriver/remote/errorhandler.py:232\u001b[39m, in \u001b[36mErrorHandler.check_response\u001b[39m\u001b[34m(self, response)\u001b[39m\n\u001b[32m    230\u001b[39m         alert_text = value[\u001b[33m\"\u001b[39m\u001b[33malert\u001b[39m\u001b[33m\"\u001b[39m].get(\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    231\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[31mWebDriverException\u001b[39m: Message: GET /session/d9f11f0f60c7b26987f7b054c1f27284/source\nBuild info: version: '3.14.0', revision: 'aacccce0', time: '2018-08-02T20:13:22.693Z'\nSystem info: host: '8d0d73b8c984', ip: '172.18.0.4', os.name: 'Linux', os.arch: 'amd64', os.version: '6.10.14-linuxkit', java.version: '1.8.0_181'\nDriver info: driver.version: unknown\nStacktrace:\n    at org.openqa.selenium.remote.http.AbstractHttpCommandCodec.decode (AbstractHttpCommandCodec.java:261)\n    at org.openqa.selenium.remote.http.AbstractHttpCommandCodec.decode (AbstractHttpCommandCodec.java:117)\n    at org.openqa.selenium.remote.server.ProtocolConverter.handle (ProtocolConverter.java:74)\n    at org.openqa.selenium.remote.server.RemoteSession.execute (RemoteSession.java:127)\n    at org.openqa.selenium.remote.server.WebDriverServlet.lambda$handle$3 (WebDriverServlet.java:250)\n    at java.util.concurrent.Executors$RunnableAdapter.call (Executors.java:511)\n    at java.util.concurrent.FutureTask.run (FutureTask.java:266)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run (Thread.java:748)"
     ]
    }
   ],
   "source": [
    "yt_inauguration_raw = driver.page_source.encode('utf-8')\n",
    "\n",
    "yt_inauguration_soup = BeautifulSoup(yt_inauguration_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The date the video was uploaded appears within a `<div id=\"date\">` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_inauguration_soup.find_all('div',{'id':'date'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digging in, we can pull out the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_inauguration_soup.find_all('div',{'id':'date'})[0].text[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The up and downvotes appear as coarse aggregations (\"16K\",\"95K\") but the true counts at the time are hidden in some \"aria-label\"s. \n",
    "\n",
    "First, find the `<button id=\"button\">` within the `<div id=\"top-level-buttons\">`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_inauguration_soup.find_all('div',{'id':'top-level-buttons'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's apparently (and hopefully only!) two of these types of divs. The one we care about is the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlb2 = yt_inauguration_soup.find_all('div',{'id':'top-level-buttons'})[1]\n",
    "\n",
    "tlb2.find_all('button',{'id':'button'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The up and down votes are the first two buttons and include an `aria-label` with a count of the number of up and down votes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upvotes = tlb2.find_all('button',{'id':'button'})[0]\n",
    "downvotes = tlb2.find_all('button',{'id':'button'})[1]\n",
    "\n",
    "upvotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the `aria-label`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upvotes['aria-label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a regex to extract the number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'([\\d,]+) other people',upvotes['aria-label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'([\\d,]+) other people',downvotes['aria-label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For closed captioned videos, there's also a transcript of the video with timestamps and text within the `<ytd-transcript-renderer>` parent or `<div class=\"cue-group\">` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_inauguration_soup.find_all('div',{'class':'cue-group'})[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the `<div class=\"cue-group\">` elements, we can extract the time code and the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_cg = yt_inauguration_soup.find_all('div',{'class':'cue-group'})[-1]\n",
    "last_cg.find_all('div',{'class':'cue-group-start-offset'})[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_cg.find_all('div',{'class':'cue'})[0].text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the whole transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg_l = []\n",
    "\n",
    "for cg in yt_inauguration_soup.find_all('div',{'class':'cue-group'}):\n",
    "    _time_code = cg.find_all('div',{'class':'cue-group-start-offset'})[0].text.strip()\n",
    "    _text = cg.find_all('div',{'class':'cue'})[0].text.strip()\n",
    "    \n",
    "    _d = {'Time':_time_code,\n",
    "          'Text':_text.replace('\\n',' ')}\n",
    "    \n",
    "    cg_l.append(_d)\n",
    "    \n",
    "pd.DataFrame(cg_l).set_index('Time')['Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping multiple videos\n",
    "That was all to extract the data from a single video. Let's now scrape the content from each of the White House YouTube videos. With something like 160 videos times 2 seconds per video, this scrape should take just over 5 minutes. So let's let this run and take a break. Something will likely break, so let's check back in afterwards! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in videos_l:\n",
    "    _link = v['Link']\n",
    "    \n",
    "    # Have Selenium get the page, get the source, and convert to soup\n",
    "    driver.get('https://www.youtube.com'+_link)\n",
    "    \n",
    "    # Give the page a second to load\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Retrieve the content and soup-ify\n",
    "    _raw = driver.page_source.encode('utf-8')\n",
    "    _soup = BeautifulSoup(_raw)\n",
    "    \n",
    "    # Get the date\n",
    "    try:\n",
    "        _date = _soup.find_all('div',{'id':'date'})[0].text[1:]\n",
    "    except:\n",
    "        # If we get an index error above, the page didn't finish loading\n",
    "        # Wait another 2 seconds\n",
    "        time.sleep(2)\n",
    "        _date = _soup.find_all('div',{'id':'date'})[0].text[1:]\n",
    "    \n",
    "    # Get the up and downvotes\n",
    "    try:\n",
    "        _tlb2 = _soup.find_all('div',{'id':'top-level-buttons'})[-1]\n",
    "        _upvotes_soup = _tlb2.find_all('button',{'id':'button'})[0]\n",
    "        _downvotes_soup = _tlb2.find_all('button',{'id':'button'})[1]\n",
    "        _upvotes = re.findall(r'([\\d,]+) other people',_upvotes_soup['aria-label'])[0]\n",
    "        _downvotes = re.findall(r'([\\d,]+) other people',_downvotes_soup['aria-label'])[0]\n",
    "    except:\n",
    "        # If we get an index error above, the page didn't finish loading\n",
    "        # Wait another 2 seconds\n",
    "        time.sleep(2)\n",
    "        _tlb2 = _soup.find_all('div',{'id':'top-level-buttons'})[-1]\n",
    "        _upvotes_soup = _tlb2.find_all('button',{'id':'button'})[0]\n",
    "        _downvotes_soup = _tlb2.find_all('button',{'id':'button'})[1]\n",
    "        _upvotes = re.findall(r'([\\d,]+) other people',_upvotes_soup['aria-label'])[0]\n",
    "        _downvotes = re.findall(r'([\\d,]+) other people',_downvotes_soup['aria-label'])[0]\n",
    "    \n",
    "    # Update the dictionary\n",
    "    v['Date'] = _date\n",
    "    v['Upvotes'] = _upvotes\n",
    "    v['Downvotes'] = _downvotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the revised `videos_l` to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitehouse_yt_df = pd.DataFrame(videos_l)\n",
    "\n",
    "# Convert the Views, Upvotes, and Downvotes columns to ints\n",
    "whitehouse_yt_df['Views'] = whitehouse_yt_df['Views'].fillna('0').str.replace(',','').astype(int)\n",
    "whitehouse_yt_df['Upvotes'] = whitehouse_yt_df['Upvotes'].fillna('0').str.replace(',','').astype(int)\n",
    "whitehouse_yt_df['Downvotes'] = whitehouse_yt_df['Downvotes'].fillna('0').str.replace(',','').astype(int)\n",
    "\n",
    "# Convert the Date to a datetime\n",
    "whitehouse_yt_df['Date'] = pd.to_datetime(whitehouse_yt_df['Date'])\n",
    "\n",
    "whitehouse_yt_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spoofing headers\n",
    "\n",
    "When we use `requests` or Selenium to get data from other web servers, each of the get requests carries some meta-data about ourselves, called [headers](https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html). These headers tell the server what kind of web browser we are, what kinds of data we can receive, *etc*. so that the server can reply with properly-formatted information. \n",
    "\n",
    "But it is also possible for the server to understand a request and refuse to fulfill it, known as a [HTTP 403 error](https://en.wikipedia.org/wiki/HTTP_403). A server's refusal to fulfill a client's request can often be traced back to the identity a client presents through its headers or a client lacking authorization to access the data (*i.e.*, you need to authenticate with the website first). In the case of `requests`, its `get` request includes default header information that identifies it as a Python script rather than a human-driven web browser.\n",
    "\n",
    "Let's make a request for an article from the NYTimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "honest_response = requests.get('https://www.nytimes.com/2019/02/03/us/politics/trump-interview-mueller.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the headers we sent with this request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'User-Agent': 'python-requests/2.32.3', 'Accept-Encoding': 'gzip, deflate, br, zstd', 'Accept': '*/*', 'Connection': 'keep-alive'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "honest_response.request.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, the 'User-Agent' string identifies this request as originating from the \"python-requests/2.21.0\" program, rather than a typical web browser. Some web servers will be configured to inspect the headers of incoming requests and refuse requests unless they are actual web browsers.\n",
    "\n",
    "We can often circumvent these filters by sending alternative headers that claim to be from a web browser as a part of our `requests.get()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary with spoofed headers for the User-Agent\n",
    "spoofed_headers = {'User-Agent':\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36\"}\n",
    "\n",
    "# Make the request with the \n",
    "nytimes_url = 'https://www.nytimes.com/2019/02/03/us/politics/trump-interview-mueller.html'\n",
    "spoofed_response = requests.get(nytimes_url,headers=spoofed_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough, the get request we sent to the NYTimes web server now includes the spoofed \"User-Agent\" string we wrote that claims our request is from a web browser. The server should now return the data we requested, even though we are not who we claimed to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36', 'Accept-Encoding': 'gzip, deflate, br, zstd', 'Accept': '*/*', 'Connection': 'keep-alive'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spoofed_response.request.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had trouble finding a website that refused \"python-requests\" connections automatically (*e.g.*, Amazon, NYTimes, etc.), but you will likely find some along the way. \n",
    "\n",
    "Spoofing headers to conceal the identity of your client to a web server is another example of how technological capabilities can overtake ethical responsibilities. The owners of a web server may have good reasons for refusing to serve content to non-web browsers (copyright, privacy, business model, *etc*.). Misrepresenting your identity to extract this data should only be done if the risks to others are small, the benefits are in the public interest, there are no other alternatives for obtaining the data, *etc*. \n",
    "\n",
    "There can be *very* real consequences for spoofing headers. Because it is such a common and relatively trivial method for circumventing server security settings, making repeated spoofed requests could result in your IP address or an IP address range (worst case, the entire university) being blocked from making requests to the server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelizing requests\n",
    "\n",
    "A third web scraping practice that warrants ethical scrutiny is parallelization. In the example of getting historical `@WhiteHouse` tweets, we launched a single browser window and \"scrolled\" until we reached the end; a process that took on the order of a minute.\n",
    "\n",
    "However, we *could* launch multiple scripts that each creates a browser windows and collect different segments of the data in parallel for us to combine the results at the end. In an API context, we *could* create multiple applications and design our requests so that each works simultaneously to get all the data. \n",
    "\n",
    "Each request imposes some cost on the server to receive, process, and return the requested data: making these requests in parallel increases the convenience and efficiency for the data scraper, but also dramatically increases the strain on the server to fulfill other clients' requests. In fact, highly-parallelized and synchronized requests can look like [denial-of-service attacks](https://en.wikipedia.org/wiki/Denial-of-service_attack) and may get your requests far more scrutiny and blowback than patiently waiting for your data to arrive in series. The ethical justifications for employing highly-parallelized scraping approaches are thin: documenting a rapidly-unfolding event before the data disappears, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
